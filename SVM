#Support Vector Machines
#library for SVM
install.packages("e1071")
library("e1071")
#random nos. for x
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
y=c(rep(-1,10),rep(1,10))
#first 10 elements of y are -1 and the rest are 1
x[y==1,] = x[y==1,] + 1 #we add 1 point to the last 10 rows of x

#let's check whether the classes are linearly separable
plot(x,col=(3-y)) #col =2 means blue, col =4 means red
#first 10 elements with y=-1 will be painted red and rest in blue
#from the plot we can see that we want the support vector to draw a distinction line between
#the red dots and the blue dots

#next we will create a dataframe of x and y values
dat= data.frame(x=x,y=as.factor(y))

library(e1071)

svmfit=svm(y~x.1+x.2,data=dat,kernel="linear",cost=10,scale=FALSE)
plot(svmfit,dat)

#what are our support vectors?
svmfit$index
summary(svmfit)
#lets change the cost, this will increase the support vectors
svm.fit=svm(y~.,data =dat, kernel="linear",cost=0.1,scale=FALSE)
plot(svm.fit,dat)
summary(svm.fit)

#best cost, will be defined by cross validation
#cross-validation using tune
set.seed(1)
tune.out=tune(svm,y~.,data=dat[train,],kernel="radical",ranges=list(cost=c(0.1,1,10,100,1000),
                                                                    gamma=c(0.5,1,2,3,4)))
summary(tune.out)
plot(tune.out)
